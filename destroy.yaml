---
- name: Gather server facts from OpenStack
  hosts: localhost
  pre_tasks:
    - name: Set variables
      set_fact:
        state: absent
  tasks:
    - name: Gather data about k8s master
      os_server_info:
        server: "{{ master_name }}"
      register: os_server_info_master_result

    - name: Update inventory with data about k8s master
      add_host:
        name: "{{ item.name }}"
        ansible_ssh_host: "{{ item.public_v4 }}"
        ansible_ssh_user: ubuntu
        groupname: master
      loop: "{{ os_server_info_master_result.openstack_servers }}"

    - name: Gather data about k8s nodes
      os_server_info:
        server: "{{ nodes_name }}*"
      register: os_server_info_nodes_result

    - name: Update inventory with data about k8s nodes
      add_host:
        name: "{{ item.name }}"
        ansible_ssh_host: "{{ item.private_v4 }}"
        ansible_ssh_user: ubuntu
        ansible_ssh_common_args: '-o ProxyCommand="ssh -W %h:%p -q ubuntu@{{ hostvars[groups.master[0]][''ansible_ssh_host''] }}"'
        groupname: nodes
      when: item.name != master_name
      loop: "{{ os_server_info_nodes_result.openstack_servers }}"

- name: Drain all worker nodes (using kubectl drain)
  hosts: master
  gather_facts: no
  ignore_unreachable: yes
  tasks:
    - name: Delete all services (e.g., to release loadbalancers in OpenStack)
      shell: "kubectl delete svc --all-namespaces --force --all"

    - name: Wait until public IPs are released
      shell: "kubectl get svc --all-namespaces -o jsonpath='{.items[*].status.loadBalancer.ingress[*].ip}'"
      register: load_balancer_public_ips
      retries: 10
      delay: 6
      until: load_balancer_public_ips.stdout | trim | length == 0

    - name: Get the nodes to drain
      # Get a list of worker nodes (separated by blanks)
      shell: "kubectl get nodes --selector='!node-role.kubernetes.io/master' -o jsonpath='{.items[*].metadata.name}'"
      register: worker_nodes_to_drain

    - name: Drain each node worker node
      shell: "kubectl drain --force --delete-local-data=true --grace-period=60 --ignore-daemonsets=true --timeout=120s '{{ item }}'"
      loop: "{{ worker_nodes_to_drain.stdout.split(' ') }}"
      ignore_errors: True

- name: Destroy k8s cluster on nodes (using kubeadm reset)
  hosts: nodes
  become: true
  gather_facts: no
  ignore_unreachable: yes
  pre_tasks:
    - name: Set variables
      set_fact:
        state: absent
  roles:
    - kubeadm-nodes

- name: Destroy k8s cluster on nodes (using kubeadm reset)
  hosts: master
  become: true
  gather_facts: no
  ignore_unreachable: yes
  ignore_errors: yes
  pre_tasks:
    - name: Set variables
      set_fact:
        state: absent
  roles:
    - kubeadm-master

- name: Destroy OpenStack cluster resources
  hosts: localhost
  pre_tasks:
    - name: Set variables
      set_fact:
        state: absent

  roles:
    - openstack-master
    - openstack-nodes
    - openstack-security-groups

  tasks:
    - name: Clear static routes
      shell: openstack router set --no-route "{{ router_name }}"

    - name: Delete router
      os_router:
        state: absent
        name: "{{ router_name }}"

    - name: Delete network
      os_network:
        state: absent
        name: "{{ network_name }}"

    - name: Delete subnet
      os_subnet:
        state: absent
        name: "{{ network_name }}"
